[model]
embed_device = "Cpu"                                       # Device to put the embed tensor ("Cpu" or "Gpu").
max_batch = 16                                             # The maximum batches that are cached on GPU.
max_runtime_batch = 8                                      # The maximum batches that can be scheduled for inference at the same time.
model_name = "RWKV-x060-World-3B-v2.1-20240417-ctx4096.st" # Name of the model.
model_path = "assets/models"                               # Path to the folder containing all models.
quant = 0                                                  # Layers to be quantized.
quant_type = "Int8"                                        # Quantization type ("Int8" or "NF4").
stop = ["\n\n"]                                            # Additional stop words in generation.
token_chunk_size = 128                                     # Size of token chunk that is inferred at once. For high end GPUs, this could be 64 or 128 (faster).

[tokenizer]
path = "assets/tokenizer/rwkv_vocab_v20230424.json" # Path to the tokenizer.

[bnf]
enable_bytes_cache = true   # Enable the cache that accelerates the expansion of certain short schemas.
start_nonterminal = "start" # The initial nonterminal of the BNF schemas.

[adapter]
Auto = {} # Choose the best GPU.
# Manual = 0 # Manually specify which GPU to use.

[listen]
acme = false
domain = "local"
ip = "0.0.0.0"   # Use IpV4.
# ip = "::"        # Use IpV6.
force_pass = true
port = 65530
slot = "permisionkey"
tls = true

[[listen.app_keys]] # Allow mutiple app keys.
app_id = "JUSTAISERVER"
secret_key = "JUSTSECRET_KEY"

# [[lora]]
# alpha = 192
# path = "assets/models/rwkv-x060-3b.lora.st"
