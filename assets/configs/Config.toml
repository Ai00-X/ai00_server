[model]
embed_device = "Cpu"                                     # Device to put the embed tensor ("Cpu" or "Gpu").
head_chunk_size = 8192                                   # DO NOT modify this if you don't know what you are doing.
max_batch = 16                                           # The maximum batches that are cached on GPU.
max_runtime_batch = 8                                    # The maximum batches that can be scheduled for inference at the same time.
model_name = "RWKV-x060-World-3B-v2-20240228-ctx4096.st" # Name of the model.
model_path = "assets/models"                             # Path to the folder containing all models.
quant = 0                                                # Layers to be quantized.
quant_type = "Int8"                                      # Quantization type ("Int8" or "NF4").
state_chunk_size = 4                                     # The chunk size of layers in model state.
stop = ["\n\n"]                                          # Additional stop words in generation.
token_chunk_size = 128                                   # Size of token chunk that is inferred at once. For high end GPUs, this could be 64 or 128 (faster).
turbo = true                                             # Whether to use alternative GEMM kernel to speed-up long prompts.

[tokenizer]
path = "assets/tokenizer/rwkv_vocab_v20230424.json" # Path to the tokenizer.

[bnf]
enable_bytes_cache = true   # Enable the cache that accelerates the expansion of certain short schemas.
start_nonterminal = "start" # The initial nonterminal of the BNF schemas.

[adapter]
Auto = {}

[listen]
acme = false
domain = "local"
ip = "0.0.0.0"   # Use IpV4.
# ip = "::"        # Use IpV6.
force_pass = true
port = 65530
slot = "permisionkey"
tls = true

[[listen.app_keys]] # Allow mutiple app keys.
app_id = "JUSTAISERVER"
secret_key = "JUSTSECRET_KEY"

# [[lora]]
# alpha = 192
# path = "assets/models/rwkv-x060-3b.lora.st"
