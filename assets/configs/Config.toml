[model]
model_name = "RWKV-x060-World-1B6-v2-20240208-ctx4096.st" # Path to the model.
model_path = "assets/models"
embed_device = "Cpu"                                             # Device to put the embed tensor ("Cpu" or "Gpu").
head_chunk_size = 8192                                           # DO NOT modify this if you don't know what you are doing.
max_batch = 16                                                   # The maximum batches that are cached on GPU.
max_runtime_batch = 8                                            # The maximum batches that can be scheduled for inference at the same time.
quant = 0                                                        # Layers to be quantized.
quant_type = "Int8"                                              # Quantization type ("Int8" or "NF4").
state_chunk_size = 4                                             # The chunk size of layers in model state.
stop = ["\n\n"]                                                  # Additional stop words in generation.
token_chunk_size = 128                                           # Size of token chunk that is inferred at once. For high end GPUs, this could be 64 or 128 (faster).
turbo = true                                                     # Whether to use alternative GEMM kernel to speed-up long prompts.

[tokenizer]
path = "assets/tokenizer/rwkv_vocab_v20230424.json" # Path to the tokenizer.

[adapter]
Auto = {}

[listen]
acme = false
domain = "local"
ip = "0.0.0.0"   # Use IpV4
# ip = "::"        # To enable IpV6
port = 65530
tls = false
